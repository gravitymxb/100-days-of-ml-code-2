#Day 1: Data Prepocessing
# 数据预处理

#Step 1: Importing the libraries
#导入库
import numpy as np
import pandas as pd #pandas = panel data s

#Step 2: Importing dataset
import sklearn

dataset = pd.read_csv('E:/GitHub clone/100-Days-Of-ML-Code/datasets/Data.csv')
#E:\GitHub clone\100-Days-Of-ML-Code\datasets
#将自变量（3列）和因变量（1列）拆开，拆成一个矩阵和一个向量。
X = dataset.iloc[ : , :-1].values
#iloc利用index的具体位置（所以它只能是整数型参数），来获取想要的行（或列）
#取除了最后一列的所有数据
Y = dataset.iloc[ : , 3].values
#取第三行的所有数据

print("Step 2: Importing dataset")
print("X")
print(X)
print("Y")
print(Y)

#Step 3: Handling the missing data
from sklearn.preprocessing import Imputer
imputer = sklearn.preprocessing.Imputer(missing_values ="NaN", strategy ="mean", axis = 0)
#axis = 0 表示按列进行
imputer = imputer.fit(X[ : , 1:3])
#fit() 训练集的固有属性
X[ : , 1:3] = imputer.transform(X[ : , 1:3])
#在Fit的基础上，进行标准化，降维，归一化等操作
print("---------------------")
print("Step 3: Handling the missing data")
print("step2")
print("X")
print(X)

#Step 4: Encoding categorical data
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
labelencoder_X = LabelEncoder()
#LabelEncoder可以将标签分配一个0—n_classes-1之间的编码 将各种标签分配一个可数的连续编号：
X[ : , 0] = labelencoder_X.fit_transform(X[ : , 0])
#Creating a dummy variable
onehotencoder = OneHotEncoder(categorical_features = [0])
#独热码，在英文文献中称做 one-hot code, 直观来说就是有多少个状态就有多少比特，而且只有一个比特为1，其他全为0的一种码制。
X = onehotencoder.fit_transform(X).toarray()
labelencoder_Y = LabelEncoder()
Y =  labelencoder_Y.fit_transform(Y)
print("---------------------")
print("Step 4: Encoding categorical data")
print("X")
print(X)
print("Y")
print(Y)

#Step 5: Splitting the datasets into training sets and Test sets
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split( X , Y , test_size = 0.2, random_state = 0)
#https://blog.csdn.net/sinat_23338865/article/details/80248599e 上述函数的用法
print("---------------------")
print("Step 5: Splitting the datasets into training sets and Test sets")
print("X_train")
print(X_train)
print("X_test")
print(X_test)
print("Y_train")
print(Y_train)
print("Y_test")
print(Y_test)

#Step 6: Feature Scaling
from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
#数据在前处理的时候，经常会涉及到数据标准化。将现有的数据通过某种关系，映射到某一空间内。常用的标准化方式是,减去平均值，然后通过标准差映射到均至为0的空间内。系统会记录每个输入参数的平均数和标准差，以便数据可以还原。
#很多ML的算法要求训练的输入参数的平均值是0并且有相同阶数的方差例如:RBF核的SVM，L1和L2正则的线性回归
#sklearn.preprocessing.StandardScaler能够轻松的实现上述功能。
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.transform(X_test)
print("---------------------")
print("Step 6: Feature Scaling")
print("X_train")
print(X_train)
print("X_test")
print(X_test)
